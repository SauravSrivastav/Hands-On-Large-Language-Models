
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_a9QvUFVCUR"
   },
   "source": [
    "<h1>🧠 AI Ke Building Blocks: Tokens Aur Embeddings 🧱</h1>\n",
    "<i>Samjho kaise AI hamari bhasha ko chote tukdon aur meaning numbers mein badalta hai</i>\n",
    "\n",
    "---\n",
    "\n",
    "Namaste dosto! 👋 Welcome back to our AI journey! Pichhle Chapter mein humne apne pehle AI dost, Phi-3, se baat ki thi aur usse joke generate karwaya tha. Mazaa aaya tha na? 😄\n",
    "\n",
    "Lekin AI model asal mein hamari bhasha ko kaise samajhta hai? Woh \"chicken\" ya \"joke\" jaise shabdon ko computer ke liye useful cheez mein kaise badalta hai? Imagine karo tum ek alien se baat kar rahe ho jisko tumhari language nahi aati, par woh numbers samajhta hai. Tumhari baaton ko numbers mein badalna padega na? AI bhi kuch aisa hi karta hai!\n",
    "\n",
    "Is Chapter mein hum Language Models ke do bahut important building blocks seekhenge:\n",
    "\n",
    "1.  **Tokens:** Jaise hamare sentences chote chote shabdon se bante hain, waise hi AI ke liye text **Tokens** se banta hai. Yeh text ke sabse chote tukde hote hain jo AI samajhta hai.\n",
    "2.  **Embeddings:** Yeh woh special numbers hain jo AI har token (ya text ke group) ke liye banata hai. Socho yeh un tokens ka \"meaning number\" hai. Jin tokens ka matlab similar hota hai, unke numbers bhi ek doosre ke paas hote hain!\n",
    "\n",
    "Yeh Chapter yeh explain karega ki kaise yeh Tokens aur Embeddings AI ko hamari language samajhne mein madad karte hain. Hum is process ko practical examples ke saath dekhenge!\n",
    "\n",
    "Yeh notebook **Hands-On Large Language Models** book ke Chapter 2 ka practical hissa hai. Ismein diye gaye codes ko chala kar tum khud dekhoge ki yeh sab kaise kaam karta hai! Har step ko ek 10 saal ka bachha bhi samajh payega, promise! 😊🚀\n",
    "\n",
    "---\n",
    "\n",
    "<a href=\"https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961\"><img src=\"https://img.shields.io/badge/Buy%20the%20Book!-grey?logo=amazon\"></a>\n",
    "<a href=\"https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/\"><img src=\"https://img.shields.io/badge/O'Reilly-white.svg?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMzQiIGhlaWdodD0iMjciIHZpZXdCb3g9IjAgMCAzNCAyNyIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGNpcmNsZSBjeD0iMTMiIGN5PSIxNCIgcj0iMTEiIHN0cm9rZT0iI0Q0MDEwMSIgc3Ryb2tlLXdpZHRoPSI0Ii8+CjxjaXJjbGUgY3g9IjMwLjUiIGN5PSIzLjUiIHI9IjMuNSIgZmlsbD0iI0Q0MDEwMSIvPgo8L3N2Zz4K\"></a>\n",
    "<a href=\"https://github.com/HandsOnLLM/Hands-On-Large-Language-Models\"><img src=\"https://img.shields.io/badge/GitHub%20Repository-black?logo=github\"></a>\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter02/Chapter%202%20-%20Tokens%20and%20Token%20Embeddings.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "This notebook is for Chapter 2 of the [Hands-On Large Language Models](https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961) book by [Jay Alammar](https://www.linkedin.com/in/jalammar) and [Maarten Grootendorst](https://www.linkedin.com/in/mgrootendorst/).\n",
    "\n",
    "---\n",
    "\n",
    "<a href=\"https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961\">\n",
    "<img src=\"https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/images/book_cover.png\" width=\"350\"/></a>\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "*   [🧠 Intro: AI Ke Building Blocks](#-ai-ke-building-blocks-tokens-aur-embeddings-🧱) (Is Chapter Ka Parichay)\n",
    "*   [🛠️ Setup Taiyaari: Magic Kit Jaisa (Optional Step)](#setup-taiyaari-magic-kit-jaisa-optional-step)\n",
    "    *   [🚀 GPU Power (Super Engine!)](#gpu-power-super-engine)\n",
    "    *   [📦 Packages Install Karna (Tools Ikhatte Karna)](#packages-install-karna-tools-ikhatte-karna)\n",
    "*   [🤖 Hamare AI Dost Ko Load Karna](#hamare-ai-dost-ko-load-karna)\n",
    "    *   [Code: Model Aur Tokenizer Load Karna](#code-model-aur-tokenizer-load-karna)\n",
    "    *   [Code: AI Se Text Generate Karna (Tokenization ka pehla roop)](#code-ai-se-text-generate-karna-tokenization-ka-pehla-roop)\n",
    "    *   [Code: Input Tokens Ke IDs Dekhna](#code-input-tokens-ke-ids-dekhna)\n",
    "    *   [Code: Har ID Ko Wapas Text Mein Badal Kar Dekhna (Tokens Ko Pehchanna)](#code-har-id-ko-wapas-text-mein-badal-kar-dekhna)\n",
    "    *   [Code: Full Output Ke IDs Dekhna](#code-full-output-ke-ids-dekhna)\n",
    "    *   [Code: Kuch IDs Ko Decode Karke Samajhna (Tokenization Aur Detail Mein)](#code-kuch-ids-ko-decode-karke-samajhna)\n",
    "*   [🧩 Alag Alag AI Tokenizers Ko Compare Karna](#alag-alag-ai-tokenizers-ko-compare-karna)\n",
    "    *   [Code: Tokenizer Compare Karne Wala Function Banana](#code-tokenizer-compare-karne-wala-function)\n",
    "    *   [Code: Test Sentence Taiyaar Karna](#code-test-sentence-taiyaar-karna)\n",
    "    *   [Code: BERT Tokenizer Se Dekhna (Uncased)](#code-bert-tokenizer-se-dekhna-uncased)\n",
    "    *   [Code: BERT Cased Tokenizer Se Dekhna](#code-bert-cased-tokenizer-se-dekhna)\n",
    "    *   [Code: GPT-2 Tokenizer Se Dekhna](#code-gpt-2-tokenizer-se-dekhna)\n",
    "    *   [Code: Flan-T5 Tokenizer Se Dekhna](#code-flan-t5-tokenizer-se-dekhna)\n",
    "    *   [Code: StarCoder2 Tokenizer Se Dekhna](#code-starcoder2-tokenizer-se-dekhna)\n",
    "    *   [Code: Phi-3 Tokenizer Se Dekhna](#code-phi-3-tokenizer-se-dekhna)\n",
    "*   [📊 Contextualized Word Embeddings (Meaning Numbers Jo Badalte Hain)](#contextualized-word-embeddings-meaning-numbers-jo-badalte-hain)\n",
    "    *   [Code: Embedding Model Load Karna Aur Embeddings Lena](#code-embedding-model-load-karna-aur-embeddings-lena)\n",
    "    *   [Code: Embedding Ke Numbers Ka Shape Dekhna](#code-embedding-ke-numbers-ka-shape-dekhna)\n",
    "    *   [Code: Embedding Model Ke Tokens Dekhna](#code-embedding-model-ke-tokens-dekhna)\n",
    "    *   [Code: Raw Embedding Numbers Dekhna](#code-raw-embedding-numbers-dekhna)\n",
    "*   [📄 Text Embeddings (Poore Sentence Ya Document Ke Liye Meaning Numbers)](#text-embeddings-poore-sentence-ya-document-ke-liye-meaning-numbers)\n",
    "    *   [Code: Sentence Embedding Model Load Karna Aur Embedding Lena](#code-sentence-embedding-model-load-karna-aur-embedding-lena)\n",
    "    *   [Code: Sentence Embedding Ke Numbers Ka Shape Dekhna](#code-sentence-embedding-ke-numbers-ka-shape-dekhna)\n",
    "*   [🕰️ Word Embeddings Jo LLMs Se Purane Hain (Non-Contextual)](#word-embeddings-jo-llms-se-purane-hain-non-contextual)\n",
    "    *   [Code: Purana Embedding Model (GloVe) Load Karna](#code-purana-embedding-model-glove-load-karna)\n",
    "    *   [Code: GloVe Se Similar Words Dhundhna](#code-glove-se-similar-words-dhundhna)\n",
    "*   [🎵 Embedding Se Song Recommend Karna (Ek Real-Life Example!)](#embedding-se-song-recommend-karna-ek-real-life-example)\n",
    "    *   [Code: Song Playlist Data Load Karna](#code-song-playlist-data-load-karna)\n",
    "    *   [Code: Sample Playlists Dekhna](#code-sample-playlists-dekhna)\n",
    "    *   [Code: Playlists Par Naya Embedding Model (Word2Vec) Train Karna](#code-playlists-par-naya-embedding-model-word2vec-train-karna)\n",
    "    *   [Code: Train Kiye Model Se Similar Song IDs Dhundhna](#code-train-kiye-model-se-similar-song-ids-dhundhna)\n",
    "    *   [Code: Original Song Ke Details Dekhna](#code-original-song-ke-details-dekhna)\n",
    "    *   [Code: Recommendations Dikhane Wala Function Banana](#code-recommendations-dikhane-wala-function-banana)\n",
    "    *   [Code: Song 2172 Ke Liye Recommendations Dekhna (Heavy Metal Example)](#code-song-2172-ke-liye-recommendations-dekhna)\n",
    "    *   [Code: Song 842 Ke Liye Recommendations Dekhna (Hip-Hop Example)](#code-song-842-ke-liye-recommendations-dekhna)\n",
    "*   [📚 Seekhe Hue Shabd (Glossary)](#seekhe-hue-shabd-glossary)\n",
    "*   [✅ Summary: Kya Seekha Aur Aage Kya?](#summary-kya-seekha-aur-aage-kya)\n",
    "\n",
    "---\n",
    "\n",
    "💡 **Picture Suggestion:** Yahan par tum ek pyaari si drawing add kar sakte ho, jismein ek bachha computer ke saamne baitha hai aur uske dimag mein text hai jo chote chote building blocks (tokens) mein toot raha hai, aur un blocks ke neeche numbers likhe hain (embeddings)।"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛠️ Setup Taiyaari: Magic Kit Jaisa (Optional Step)\n",
    "\n",
    "Agar tum yeh notebook Google Colab par chala rahe ho (jahan computers cloud mein hote hain, jaise tumhara Google Drive files online save karta hai!), toh tumhe kuch special \"tools\" (packages) install karne padenge. Yeh tools AI model aur embeddings ke saath kaam karne ke liye zaroori hain. Socho yeh ek science/magic kit ke auzaar hain jo humein experiment shuru karne se pehle jama karne hain!\n",
    "\n",
    "Agar tum pehli baar Colab mein yeh notebook chala rahe ho, toh is code block ke aage se `#` hata do aur isko run karo. Yeh tools tumhare Colab computer mein daal dega.\n",
    "\n",
    "---\n",
    "\n",
    "💡 **NOTE**: Hum AI models aur embeddings ko tez chalaane ke liye **GPU** (Super Engine!) use karna chahte hain. Google Colab mein upar menu mein jao:\n",
    "**Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**.\n",
    "\n",
    "Yeh step bahut important hai speed ke liye! GPU ek special computer chip hai jo math calculations bahut tez kar sakti hai, aur AI ko bahut saari math karni padti hai!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture # Yeh ek special command hai jo is code block ka saara output chhupa deta hai.\n",
    "#             # Jab packages install hote hain, toh bahut saari lines screen par print hoti hain,\n",
    "#             # %%capture unko dikhne nahi deta, taaki screen saaf rahe.\n",
    "\n",
    "# !pip install transformers>=4.41.2 sentence-transformers>=3.0.1 gensim>=4.3.2 scikit-learn>=1.5.0 accelerate>=0.31.0\n",
    "# !        # Yeh exclamation mark (!) Colab ko bataata hai ki yeh ek command line command hai,\n",
    "#          # na ki Python code.\n",
    "# pip install # Yeh command naye software packages ko internet se download karke install karti hai.\n",
    "# transformers>=4.41.2  # Yeh AI models ke liye sabse important tool hai (Hugging Face Transformers library).\n",
    "#                      # Version 4.41.2 ya usse naya chahiye.\n",
    "#                      # Ismein AI brain aur translator (model aur tokenizer) load karne ke blueprints hain.\n",
    "# sentence-transformers>=3.0.1 # Yeh ek aur tool hai jo specially poore sentence ke liye\n",
    "#                              # meaning numbers (Text Embeddings) banane mein help karta hai.\n",
    "#                              # Version 3.0.1 ya naya chahiye.\n",
    "# gensim>=4.3.2        # Yeh tool purane tarike ke word embeddings (jaise Word2Vec, GloVe)\n",
    "#                      # aur playlist analysis jaise tasks ke liye useful hai.\n",
    "#                      # Version 4.3.2 ya naya chahiye.\n",
    "# scikit-learn>=1.5.0  # Yeh ek general machine learning tool hai.\n",
    "#                      # Shayad kuch dusre tools internally iska thoda sa use karein.\n",
    "#                      # Version 1.5.0 ya naya chahiye.\n",
    "# accelerate>=0.31.0   # Yeh AI models ko GPU (Super Engine!) par aur bhi tez chalane mein help karta hai.\n",
    "#                      # Version 0.31.0 ya naya chahiye.\n",
    "\n",
    "# Summary: Yeh block saare zaroori tools (libraries) ko hamare Colab environment mein install karta hai,\n",
    "#          taaki hum AI experiments kar sakein. Jaise science kit ke tools ko box se nikal kar arrange karna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQHfpqT_t9-K"
   },
   "source": [
    "## 🤖 Hamare AI Dost Ko Load Karna\n",
    "\n",
    "Pichhle chapter ki tarah, sabse pehle hum apna AI dost (model) aur uska translator (tokenizer) load karenge. Tokenizer woh special tool hai jo hamari bhasha ko AI samajhne wale numbers mein badalta hai, aur AI model un numbers par kaam karke naye numbers banata hai (jo phir translator wapas hamari bhasha mein badalta hai). Is chapter mein hum unko alag alag rakhenge aur tokenizer par zyada dhyan denge taaki hum dekh sakein ki yeh kaise kaam karta hai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 753
    },
    "executionInfo": null,
    "outputs": [],
    "source": [
     "# transformers library se AI model aur uska translator (tokenizer) load karne ke blueprints laate hain.\n",
     "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
     "\n",
     "# Ab hamara AI dost (model) load karte hain.\n",
     "# AutoModelForCausalLM ka matlab hai ki yeh model text generate kar sakta hai (jaise kahani banana ya jawab dena).\n",
     "# \"microsoft/Phi-3-mini-4k-instruct\": Yeh is specific AI model ka naam hai jisko hum load karna chahte hain.\n",
     "# from_pretrained matlab ki yeh model pehle se internet par train kiya hua rakha hai, usko download karke abhi use karo.\n",
     "# device_map=\"cuda\": AI calculations ko Super Engine (GPU) par chalao taaki yeh bahut tez ho. Agar GPU nahi hai toh \"auto\" use kar sakte hain jo CPU ya GPU mein se ek choose karega.\n",
     "# torch_dtype=\"auto\": Computer ko khud decide karne do ki numbers ko kaise store karna hai taaki performance achhi ho.\n",
     "# trust_remote_code=False: Security ke liye ek setting hai ki hum bharosa nahi kar rahe ki download kiye hue code mein kuch galat ho (standard practice).\n",
     "model = AutoModelForCausalLM.from_pretrained(\n",
     "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
     "    device_map=\"cuda\",\n",
     "    torch_dtype=\"auto\",\n",
     "    trust_remote_code=False,\n",
     ")\n",
     "\n",
     "# Ab AI dost ka personal translator (tokenizer) load karte hain.\n",
     "# AutoTokenizer automatically sahi type ka translator dhundh leta hai.\n",
     "# \"microsoft/Phi-3-mini-4k-instruct\": Translator ka naam bhi model ke naam jaisa hi hai taaki yeh sahi match ho.\n",
     "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
     "\n",
     "# Yei! Hamara AI dost aur uska personal translator ab ready hain! 🎉\n",
     "# Is step mein thoda time lag sakta hai kyunki model internet se download hota hai (jaise koi bada video download ho raha ho)."
    ]
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yeh Code Kya Karta Hai? 🤔**\n",
    "\n",
    "Yeh code `transformers` naam ki library se hamara AI dost (`model`) aur uska translator (`tokenizer`) load karta hai. `AutoModelForCausalLM` hamara AI brain load karta hai jo text bana sakta hai (jaise kahani, email, jawab). `AutoTokenizer` woh tool load karta hai jo hamari English/Hinglish text ko numbers mein badalta hai jo AI brain samajhta hai. Hum `from_pretrained` use karke pehle se train kiya hua (smart) AI model download karte hain aur `device_map=\"cuda\"` se usko Super Engine (GPU) par daalte hain taaki woh tez kaam kare. Yeh sab taiyaari hai AI se baat karne ke liye!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: AI Se Text Generate Karna (Tokenization ka pehla roop)\n",
    "\n",
    "Chalo, ab hamare AI dost ko ek kaam dete hain. Hum usse ek email likhne ko bolenge. Dekhte hain ki hamari baat (prompt) AI tak kaise pahunchti hai numbers mein, aur AI ka jawab numbers se wapas text mein kaise aata hai।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": null,
    "outputs": [],
    "source": [
     "# Yeh woh instruction (prompt) hai jo hum AI model ko denge. Jaise tum teacher se sawal poochte ho.\n",
     "# Hum email likhne ko bol rahe hain.\n",
     "# \"<|assistant|>\" yeh model ko bataata hai ki tumhari baat khatam ho gayi, ab AI ki baari hai jawab dene ki.\n",
     "prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.<|assistant|>\"\n",
     "\n",
     "# Ab hum is prompt (hamari bhasha) ko AI samajhne wali language (numbers) mein badlenge, using hamare translator (tokenizer).\n",
     "# tokenizer(prompt, return_tensors=\"pt\"): Translator ko prompt do aur bolo ki isko computer friendly numbers (PyTorch tensor) mein badal de.\n",
     "# .input_ids: Humein sirf woh numbers chahiye jo text ko represent karte hain (IDs), baki extra info abhi nahi chahiye.\n",
     "# .to(\"cuda\"): In numbers ko Super Engine (GPU) par bhej do, taaki AI unko tez process kar sake. Agar GPU use nahi kar rahe toh '.to(\"cuda\")' hata dena ya '.to(\"cpu\")' kar dena.\n",
     "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
     "\n",
     "# Ab hum AI model (brain) ko yeh numbers (input_ids) denge aur usse naya text generate karwayenge.\n",
     "# model.generate(): AI brain ko bolo ki ab tumhari baari hai, in numbers ko padho aur aage ki baat banao.\n",
     "# input_ids=input_ids: Yeh numbers hamare sawal/instruction hain jo AI ko samajhne hain.\n",
     "# max_new_tokens=20: Zyada se zyada 20 naye tokens (shabd ya shabdon ke tukde) banao. Hum thoda chota output mang rahe hain abhi.\n",
     "generation_output = model.generate(\n",
     "  input_ids=input_ids,\n",
     "  max_new_tokens=20\n",
     ")\n",
     "\n",
     "# Ab jo naye numbers bane hain (generation_output), unko wapas hamari bhasha (text) mein badlenge, wapas translator (tokenizer) use karke.\n",
     "# tokenizer.decode(): Translator ko numbers do aur bolo ki isko wapas English/Hinglish text mein badal do.\n",
     "# generation_output[0]: generation_output mein output ek list ke andar hota hai, uska pehla item lo (jo hamara actual generated text hai numbers mein).\n",
     "# print(): Final text ko screen par dikha do!\n",
     "print(tokenizer.decode(generation_output[0]))\n",
     "\n",
     "# Output mein dekho, AI ne hamara prompt padha aur email ka Subject aur \"Dear\" likhna shuru kar diya hai!\n",
     "# Ismein shuru mein \"<s>\" jaisa kuch dikh sakta hai, yeh special token hai jo sentence ki shuruaat batata hai."
    ]
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yeh Code Kya Karta Hai? 🤔**\n",
    "\n",
    "Yeh code dikhata hai ki kaise hum ek instruction (`prompt`) ko `tokenizer` use karke AI-friendly numbers (`input_ids`) mein badalte hain। Socho yeh hamari bhasha ko AI ki bhasha mein translate kar raha hai। Phir AI model `model.generate()` function use karke un numbers se naye numbers banata hai (jaise AI brain kuch soch raha ho aur uske dimag mein numbers chal rahe hon)। Aur finally, `tokenizer.decode()` se un naye numbers ko wapas hamari samajh mein aane wali bhasha (text) mein badal dete hain। Yeh hai Language Models ke kaam karne ka core idea - text ko numbers, numbers ko naye numbers, aur naye numbers ko wapas text mein badalna!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: Input Tokens Ke IDs Dekhna\n",
    "\n",
    "Chalo ab dekhte hain ki hamara original prompt \"Write an email apologizing...\" numbers mein kaisa dikhta hai, jab tokenizer ne usko badla tha. Har number ek token ko represent karta hai।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": null,
    "outputs": [],
    "source": [
     "# Ab hum dekhte hain ki hamara prompt \"Write an email...\" numbers (input_ids) mein kaisa dikhta hai.\n",
     "# print(input_ids) se hum woh numbers screen par dikhaate hain.\n",
     "# Yeh numbers PyTorch tensor format mein hain, jo computer ke liye ek special number grid jaisa hai.\n",
     "# Har number is tensor mein ek chote shabd ya character group ko represent karta hai (jisko \"token\" kehte hain).\n",
     "# tensor([[...]]) mein pehli list hamare sentence ke numbers hain.\n",
     "print(input_ids)\n",
     "\n",
     "# Output mein woh lambi list computer ke numbers hain! Yahi numbers AI model process karta hai."
    ]
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yeh Code Kya Karta Hai? 🤔**\n",
    "\n",
    "Yeh code dikhata hai ki hamare prompt (`input_ids`) ko tokenizer ne kin numbers mein badla hai। Socho yeh AI brain ki dictionary ke codes hain। Computer ke liye hamari bhasha numbers mein store hoti hai। Har number ek **Token ID** hai, jo ek specific token (chota text ka tukda) ko represent karta hai। Yeh woh pehla step hai jab text computer ke andar jaata hai।"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: Har ID Ko Wapas Text Mein Badal Kar Dekhna (Tokens Ko Pehchanna)\n",
    "\n",
    "Ab dekhte hain ki woh numbers wapas text mein kaise badalte hain, ek ek token karke! Jaise numbers ko decode karke dekhein ki har number kis LEGO piece ka code hai।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": null,
    "outputs": [],
    "source": [
     "# Ab hum wohi numbers (input_ids) lenge jo pichhle step mein bane the.\n",
     "# input_ids[0] se hum pehle (aur yahan ek hi) sentence ke numbers ki list lete hain.\n",
     "# for id in input_ids[0]: is list mein har number (jisko hum 'id' bol rahe hain) par jao ek-ek karke.\n",
     "# print(tokenizer.decode(id)): Hamare translator (tokenizer) se poochho ki yeh 'id' number kis text ke tukde ko represent karta hai, aur us text ko screen par dikhao.\n",
     "\n",
     "for id in input_ids[0]:\n",
     "   print(tokenizer.decode(id))\n",
     "\n",
     "# Output mein dekho, hamara sentence \"Write an email...\" kaise chote chote tokens mein toot gaya hai!\n",
     "# Kuch token poore words hain (\"Write\", \"an\", \"email\").\n",
     "# Lekin kuch words tukdon mein hain (\"apolog\", \"izing\" milakar \"apologizing\" banta hai; \"trag\", \"ic\" milakar \"tragic\").\n",
     "# Punctuation ('.') bhi alag token hai.\n",
     "# Special tokens bhi hain jaise \"<s>\" (sentence start) aur \"<|assistant|>\" (AI ki baari ka token jo humne prompt mein dala tha).\n",
     "# Yahi hain woh **Tokens**!"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yeh Code Kya Karta Hai? 🤔**\n",
    "\n",
    "Yeh code dikhata hai ki hamare prompt ke numbers (Token IDs) ko agar wapas text mein badlein ek ek karke, toh kya banta hai. Yeh woh **Tokens** hain jinmein tokenizer ne hamare original text ko toda tha. Socho yeh hamare sentence ke LEGO pieces hain! Kuch tokens poore shabd hote hain, aur kuch shabdon ke tukde ya punctuation hote hain। Is process ko **Tokenization** kehte hain।"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: Full Output Ke IDs Dekhna\n",
    "\n",
    "Jab AI jawab generate karta hai, toh woh bhi numbers mein hi hota hai। Pehle chapter mein humne dekha tha ki AI numbers generate karta hai aur phir woh numbers wapas text mein badal jaate hain। Chalo ab uske numbers dekhte hain।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": null,
    "outputs": [],
    "source": [
     "# Ab dekhte hain ki AI ne jo naya text generate kiya (generation_output), woh numbers mein kaisa dikhta hai.\n",
     "# Ismein hamare original input numbers bhi shamil hote hain shuru mein, aur phir AI ke banaye naye numbers.\n",
     "# generation_output variable ko print karne se uska content dikhta hai.\n",
     "# Yeh bhi ek PyTorch tensor hai.\n",
     "generation_output\n",
     "\n",
     "# Output mein dekho, input numbers ke baad kuch naye numbers aa gaye hain.\n",
     "# Shuru ke numbers wohi hain jo input_ids mein the.\n",
     "# Baad ke numbers AI ka banaya hua jawab hai, numbers ke roop mein!"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yeh Code Kya Karta Hai? 🤔**\n",
    "\n",
    "Yeh code `generation_output` variable ko dikhata hai, jismein AI model ka poora output store hai, numbers (Token IDs) ke roop mein। Is list mein shuru ke IDs wohi hain jo input mein the (hamara prompt), aur baad ke IDs woh hain jo model ne generate kiye hain (jo naya text humne output mein dekha tha)। AI brain aise hi numbers ki lists ke saath kaam karta hai।"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: Kuch IDs Ko Decode Karke Samajhna (Tokenization Aur Detail Mein)\n",
    "\n",
    "AI ne jo numbers generate kiye hain, unmein se kuch IDs ko decode karke dekhte hain ki unka kya matlab hai। Isse tokenization process aur clear ho jayega।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": null,
    "outputs": [],
    "source": [
     "# Tokenizer mein kuch IDs ko individually decode karke dekhte hain ki woh kya text banate hain.\n",
     "# Dekhte hain ID 3323 kya hai? (Yeh ek ID hai jo humne generation_output numbers mein dekha tha, \"Subject\" ke shuru mein)\n",
     "print(tokenizer.decode(3323))\n",
     "\n",
     "# Dekhte hain ID 622 kya hai? (Yeh bhi generation_output numbers mein tha, 3323 ke baad)\n",
     "print(tokenizer.decode(622))\n",
     "\n",
     "# Ab dono IDs ko saath mein decode karte hain (tokenizer.decode() ek single ID ya list of IDs le sakta hai).\n",
     "# Dekhte hain kya pura word banta hai?\n",
     "print(tokenizer.decode([3323, 622]))\n",
     "\n",
     "# Ek aur ID dekhte hain, 29901. (Yeh bhi output numbers mein tha, \"Subject:\" ke ':' ke liye)\n",
     "print(tokenizer.decode(29901))\n",
     "\n",
     "# Output mein dekho, \"Subject\" do alag alag IDs se bana hai (\"Sub\" aur \"ject\").\n",
     "# Agar hum un dono IDs ko saath mein decode karte hain, toh tokenizer unko join karke \"Subject\" bana deta hai.\n",
     "# Colon (\":\") ka bhi ek alag ID hai.\n",
     "# Yeh dikhata hai ki tokenizer words ko kaise chote chote tukdon (sub-word tokens) mein todta hai (tokenization)."
    ]
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yeh Code Kya Karta Hai? 🤔**\n",
    "\n",
    "Yeh code kuch specific Token IDs (jo humne generated output mein dekhe the) ko lekar `tokenizer.decode()` function use karke unka text dikhata hai। Isse hum dekh sakte hain ki kaise kuch words (`Subject`) ek se zyada tokens mein divide hote hain (`Sub` aur `ject`), aur kaise punctuation marks (:) bhi alag tokens ho sakte hain। Jab in tokens ko wapas decode kiya jaata hai, toh yeh milkar original text bana dete hain। Yahi hai **Tokenization** process ko andar se dekhna!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9nRducW48bd"
   },
   "source": [
    "## 🧩 Alag Alag AI Tokenizers Ko Compare Karna\n",
    "\n",
    "Jaise har insaan thodi alag bhasha bolta aur samajhta hai (ya ek hi bhasha ke alag alag accent hote hain!), waise hi alag alag AI models ke **Tokenizers** bhi thode alag ho sakte hain. Ek tokenizer text ko ek tarike se tod sakta hai, aur doosra thoda alag tarike se. Yeh ispar depend karta hai ki model ko kaise train kiya gaya hai aur kaunsi **Tokenization strategy** use ki gayi hai (rules for breaking text into tokens)।\n",
    "\n",
    "Chalo dekhte hain ki ek hi sentence ko alag alag popular AI models ke tokenizers kaise break karte hain. Isko aasan banane ke liye, hum ek chota sa helper function banayenge jo humein colorful output dikhayega!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7W0xFIVo5A0S"
   },
   "outputs": [],
   "source": [
    "# Zaroori tools laate hain transformers library se - abhi model nahi, sirf tokenizer ke blueprints chahiye!\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Kuch colors define karte hain, taaki jab hum tokens dekhein toh woh colorful dikhein! ✨\n",
    "# Yeh sirf dikhane ke liye hai, asli code ke liye zaroori nahi.\n",
    "# Yeh numbers RGB values hain jo computer screen par colors banate hain.\n",
    "colors_list = [\n",
    "    '102;194;165', # Color 1 (Greenish)\n",
    "    '252;141;98',  # Color 2 (Orangish)\n",
    "    '141;160;203', # Color 3 (Blueish)\n",
    "    '231;138;195', # Color 4 (Pinkish)\n",
    "    '166;216;84',  # Color 5 (Light Green)\n",
    "    '255;217;47'   # Color 6 (Yellowish)\n",
    "]\n",
    "\n",
    "# Yeh ek function banate hain jo humein dikhayega ki koi sentence tokens mein kaise toot ta hai.\n",
    "# def show_tokens(sentence, tokenizer_name): matlab ek function bana rahe hain jiska naam show_tokens hai,\n",
    "#                                          aur woh do cheezein input lega: sentence aur tokenizer_name.\n",
    "def show_tokens(sentence, tokenizer_name):\n",
    "    print(f\"\\n--- Tokenizer: {tokenizer_name} ---\") # Screen par print karo ki hum kaunsa tokenizer use kar rahe hain.\n",
    "    # Us naam ka tokenizer load karte hain.\n",
    "    # AutoTokenizer automatically sahi type ka tokenizer internet se dhundh kar download (agar nahi hai) aur load kar leta hai naam se.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    # Ab sentence ko tokens (numbers) mein badalte hain using the loaded tokenizer.\n",
    "    # tokenizer(sentence) yeh kam karta hai.\n",
    "    # .input_ids se sirf woh numbers ki list milti hai jo text ko represent karte hain (Token IDs).\n",
    "    token_ids = tokenizer(sentence).input_ids\n",
    "    # Ab har token ID par jao list mein ek-ek karke.\n",
    "    # enumerate(token_ids) se humein har number (id) aur uska index (position) dono milte hain. 'idx' position hai, 't' number (id) hai.\n",
    "    for idx, t in enumerate(token_ids):\n",
    "        # Ab har token ko ek unique color mein print karte hain.\n",
    "        # f'...': Yeh ek special way hai text print karne ka jismein hum variables ko `{}` ke andar daal sakte hain.\n",
    "        # '\\x1b[0;30;48;2;{colors_list[idx % len(colors_list)]}m': Yeh kuch special codes hain jo terminal ko batate hain ki ab jo text print hoga, usko is color mein rang do.\n",
    "        # colors_list[idx % len(colors_list)] yeh logic use karta hai position (idx) ko hamare colors list ki lambai se divide karke remainder lena (% modulo operator). Isse colors repeat hote rehte hain agar bahut saare tokens hon.\n",
    "        # tokenizer.decode(t): Yeh is current number (token ID 't') ko wapas text mein badalta hai (woh chota tukda).\n",
    "        # '\\x1b[0m': Yeh codes terminal ko batate hain ki ab coloring band kar do, normal black text par wapas aa jao.\n",
    "        # end=' ': normally print() ke baad naya line shuru hota hai, end=' ' se hum keh rahe hain ki naya line nahi, ek space do, taaki saare tokens ek hi line mein dikhein.\n",
    "        print(\n",
    "            f'\\x1b[0;30;48;2;{colors_list[idx % len(colors_list)]}m' + # Color start code (background color change karta hai)\n",
    "            tokenizer.decode(t) + # Actual token text jo color mein dikhega\n",
    "            '\\x1b[0m', # Color end code (reset formatting)\n",
    "            end=' ' # Token ke baad ek space\n",
    "        )\n",
    "    # Jab saare tokens print ho jaayein, ek naya line shuru karte hain taaki agla output saaf dikhe.\n",
    "    print()\n",
    "\n",
    "# Function ab memory mein save ho gaya hai aur use hone ke liye taiyaar hai!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yeh Code Kya Karta Hai? 🤔**\n",
    "\n",
    "Yeh code ek helper function `show_tokens` banata hai. Socho yeh ek special magnifying glass jaisa hai jisse hum kisi bhi sentence ko, alag alag AI model ke translator (tokenizer) ke nazar se dekh sakte hain. Is function ka kaam hai ki tum usko koi sentence aur kisi bhi AI model ke tokenizer ka naam do, aur woh tumhe dikhayega ki woh particular tokenizer us sentence ko kaise chote chote **Tokens** mein todta hai. Ismein colors ka use kiya gaya hai taaki har token alag alag dikhe, samajhna aasan ho jaye! Yeh function chapter ke alag alag tokenizers ko compare karne mein bahut help karega।"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: Test Sentence Taiyaar Karna\n",
    "\n",
    "Ab hum ek sentence banayenge jismein alag alag tarah ki cheezein hain, jaise bade akshar, chhote akshar, symbols, numbers, spaces, aur doosri bhasha ke characters. Aisa sentence lene se hum dekh payenge ki har tokenizer in sab cheezon ko kaise handle karta hai।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gcc3JjwX5DK-"
   },
   "outputs": [],
   "source": [
    "# Yeh hamara test sentence hai jisko hum alag alag tokenizers se check karenge.\n",
    "# Ismein alag alag tarah ki cheezein hain, jaise ek 'obstacle course' ho tokenizers ke liye:\n",
    "# - Normal English words: \"English\", \"and\"\n",
    "# - CAPITALIZATION: \"CAPITALIZATION\" - bade akshar mein\n",
    "# - Emojis: \"🎵\" - music emoji\n",
    "# - Non-English characters: \"鸟\" - Chinese word for bird\n",
    "# - Programming keywords: \"False\", \"None\", \"elif\", \"else\" - jo code mein use hote hain\n",
    "# - Operators: \"==\", \">=\" - jo code mein use hote hain\n",
    "# - Punctuation: \":\", \"\\\"\"\n",
    "# - Spaces, including multiple spaces and tabs: \"    \" (4 spaces) \"       \" (7 spaces) - yeh dekhenge ki spaces ko kaise treat karte hain\n",
    "# - Numbers and math operators: \"12.0*50=600\"\n",
    "\n",
    "# Triple quotes (\"\"\") use karte hain jab text kayi lines ka ho.\n",
    "text = \"\"\"\n",
    "English and CAPITALIZATION\n",
    "🎵 鸟\n",
    "show_tokens False None elif == >= else: two tabs:\"    \" Three tabs: \"       \"\n",
    "12.0*50=600\n",
    "\"\"\"\n",
    "\n",
    "# Yeh sentence humein dikhayega ki kaise different tokenizers letters, symbols, aur numbers ko handle karte hain.\n",
    "# Har tokenizer ka apna tarika hoga is 'obstacle course' ko tackle karne ka!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yeh Code Kya Karta Hai? 🤔**\n",
    "\n",
    "Yeh code ek multi-line string variable `text` banata hai. Is string mein humne jaan boojh kar alag alag tarah ke characters, words (capital letters mein bhi), symbols (emoji aur Chinese character), numbers, operators, aur spaces (tabs) dale hain. Socho yeh ek special challenge text hai! Hum is sentence ko use karke dekhenge ki alag alag tokenizers in sab cheezों ko kaise tokens mein todte hain. Har tokenizer ka apna tarika hota hai!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: BERT Tokenizer Se Dekhna (Uncased)\n",
    "\n",
    "Chalo, ab hamara `show_tokens` function use karke dekhte hain ki Google ke **BERT** model ka \"uncased\" tokenizer is sentence ko kaise tokens mein todta hai. \"Uncased\" matlab yeh bade aksharon ko chote aksharon mein badal deta hai। Socho yeh ek aisa translator hai jo sab kuch lower case mein likhta hai।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": null,
    "outputs": [],
    "source": [
     "# show_tokens function ko hamara test text aur BERT uncased tokenizer ka naam do.\n",
     "# Pehle input hai hamara 'text' variable.\n",
     "# Doosra input hai tokenizer ka naam: \"bert-base-uncased\" - yeh Hugging Face models mein ek standard naam hai BERT ke uncased version ke liye.\n",
     "# \"uncased\" matlab yeh input ke bade letters ko chota kar deta hai, jaise \"ENGLISH\" ban jayega \"english\".\n",
     "show_tokens(text, \"bert-base-uncased\")\n",
     "\n",
     "# Output dekho! Har rang ek alag token hai jo BERT ke uncased tokenizer ne banaya hai.\n",
     "# Notice karo \"English\" aur \"CAPITALIZATION\" ab chote letters mein aa gaye hain.\n",
     "# \"CAPITALIZATION\" jaisa lamba word tukdon mein toot gaya hai (jaise \"capital\" aur \"##ization\").\n",
     "# ## ka matlab hai ki yeh token pichhle token ka continuation hai (uske saath join karke padhna hai).\n",
     "# Emojis (🎵) aur Chinese characters (鸟) ke liye \"[UNK]\" tokens aa rahe hain (Unknown tokens) - matlab is tokenizer ko yeh pehchanta nahi.\n",
     "# Special tokens jaise [CLS] (Classify) aur [SEP] (Separator) bhi shuru aur end mein dikh rahe hain BERT ke liye - yeh model ke training ka part hai.\n",
     "# Tabs (multiple spaces) ke liye \"\\t\" aa raha hai - yeh tab character ko represent karta hai.\n",
     "# Yeh is tokenizer ka tarika hai text ko numbers mein badalne ka."
    ]
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yeh Code Kya Karta Hai? 🤔**\n",
    "\n",
    "Yeh code hamare test `text` ko Google ke BERT model ke \"uncased\" tokenizer se process karke dikhata hai. Socho yeh BERT ke uncased translator se text ko tod kar dekh rahe hain। \"Uncased\" ka matlab hai ki yeh tokenizer bade letters (CAPITALIZATION) ko chote letters mein badal deta hai. Output mein dekho, \"English\" aur \"CAPITALIZATION\" chote letters mein aa rahe hain। Saath hi, kuch tokens ke aage `##` laga hai, iska matlab woh kisi bade shabd ka hissa hain. Aur jo symbols aur Chinese character hain, unko yeh `[UNK]` (Unknown) token bana deta hai, kyunki yeh tokenizer unhe pehchanta nahi. Tabs (`    `) ko yeh `\\t` (tab character) mein badal deta hai. Yeh tokenizer sentence ke shuru aur end mein special tokens `[CLS]` aur `[SEP]` bhi add karta hai।"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: BERT Cased Tokenizer Se Dekhna\n",
    "\n",
    "Ab Google ke BERT model ka \"cased\" tokenizer dekhte hain. \"Cased\" matlab yeh bade aksharon ko vaise hi rakhta hai, badalta nahi। Socho yeh ek aisa translator hai jo letters ka case (bada ya chota) yaad rakhta hai।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "executionInfo": null,
    "outputs": [],
    "source": [
     "# show_tokens function ko hamara text aur BERT cased tokenizer ka naam do.\n",
     "# \"bert-base-cased\" Hugging Face mein BERT ke cased version ka naam hai.\n",
     "# \"cased\" wala version capital letters ko maintain rakhta hai, unhe chota nahi karta.\n",
     "show_tokens(text, \"bert-base-cased\")\n",
     "\n",
     "# Output dekho! Ab \"English\" aur \"CAPITALIZATION\" mein bade letters vaise hi hain.\n",
     "# \"CAPITALIZATION\" ab aur bhi zyada tukdon mein toot gaya hai (CA, ##PI, ##TA, ##L, ##I, ##Z, ##AT, ##ION) - kyunki ab case bhi matter karta hai.\n",
     "# Baki chizein uncased jaisi hi hain - [UNK] for unknown chars, \\t for tab.\n",
     "# [CLS] aur [SEP] special tokens bhi hain.\n",
     "# Yeh is tokenizer ka thoda alag aur case-sensitive tarika hai."
    ]
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yeh Code Kya Karta Hai? 🤔**\n",
    "\n",
    "Yeh code hamare test `text` ko Google ke BERT model ke \"cased\" tokenizer se process karke dikhata hai. \"Cased\" ka matlab hai ki yeh tokenizer bade letters (CAPITALIZATION) ko alag tarike se treat karta hai, unhe chote letters mein *nahi* badalta। Output mein dekho, \"English\" aur \"CAPITALIZATION\" ab tukdon mein hain aur unke capital letters preserve hue hain। Yeh tokenizer bhi `##` use karta hai aur special characters/emojis ke liye `[UNK]` dikhata hai। Tabs (`    `) ko yeh `\\t` (tab character) mein badal deta hai. Yeh tokenizer bhi `[CLS]` aur `[SEP]` tokens add karta hai। Iska matlab hai ki case (bada ya chota akshar) is tokenizer ke liye important hai।"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: GPT-2 Tokenizer Se Dekhna\n",
    "\n",
    "Ab OpenAI ke **GPT-2** model ka tokenizer dekhte hain। Yeh Language Models ki duniya mein bahut famous hai। Yeh thoda alag tarika use karta hai jisko Byte Pair Encoding (BPE) kehte hain, aur yeh spaces ko bhi words ka part treat karta hai।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "executionInfo": null,
    "outputs": [],
    "source": [
     "# show_tokens function ko hamara text aur GPT-2 tokenizer ka naam do.\n",
     "# \"gpt2\" OpenAI ka bahut popular Language Model hai.\n",
     "# Iska tokenizer Byte Pair Encoding (BPE) use karta hai (yeh tokenization strategy ka ek naam hai).\n",
     "# Iska ek khas baat hai ki yeh spaces ko word ka part treat karta hai (dekhna output mein).\n",
     "show_tokens(text, \"gpt2\")\n",
     "\n",
     "# Output dekho! Yeh tokenizer BERT se alag hai:\n",
     "# - Sabse pehle, yeh word ke aage space ko bhi ek token bana deta hai (bahut se tokens space \" \" se shuru ho rahe hain).\n",
     "# - CAPITALIZATION ab ek saath hai, BERT cased jaisa bahut saare tukdon mein nahi toot raha.\n",
     "# - Special characters/emojis (🎵, 鸟) ke liye yahan ek alag hi symbol � aa raha hai (Unicode replacement character).\n",
     "# - Tabs (multiple spaces) bhi multiple space tokens mein toot rahe hain, na ki '\\t' jaisa BERT mein.\n",
     "# Har tokenizer ka apna signature style hai! GPT-2 ka space handling BERT se alag hai, aur special chars handling bhi."
    ]
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yeh Code Kya Karta Hai? 🤔**\n",
    "\n",
    "Yeh code hamare test `text` ko OpenAI ke GPT-2 model ke tokenizer se process karke dikhata hai. Socho yeh GPT-2 ke translator se text ko tod kar dekh rahe hain। Yeh words ke aage space ko bhi ek alag token bana deta hai (output mein dekho, bahut saare tokens space se shuru ho rahe hain)। Yeh bhi words ko tukdon mein todta hai par alag tarike se, aur special characters/emojis ke liye `�` jaisa token dikhata hai। Tabs (`    `) ko bhi spaces ke tokens mein tod deta hai. Is tokenizer mein BERT jaise `[CLS]` ya `[SEP]` special tokens nahi hote."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: Flan-T5 Tokenizer Se Dekhna\n",
    "\n",
    "Ab Google ke ek aur popular model, **Flan-T5**, ka tokenizer dekhte hain। Yeh bhi ek powerful AI model hai aur iska tokenizer SentencePiece naam ki technique use karta hai।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": null,
    "outputs": [],
    "source": [
     "# show_tokens function ko hamara text aur Flan-T5 tokenizer ka naam do.\n",
     "# \"google/flan-t5-small\" Google ka ek encoder-decoder model hai (iske baare mein aage padhenge).\n",
     "# Iska tokenizer SentencePiece use karta hai - yeh bhi ek popular tokenization technique hai.\n",
     "show_tokens(text, \"google/flan-t5-small\")\n",
     "\n",
     "# Output dekho! Yeh tokenizer bhi GPT-2 jaisa hai:\n",
     "# - Spaces ko word ka part treat karta hai (tokens ke shuru mein space).\n",
     "# - CAPITALIZATION ko maintain rakhta hai aur bade shabdon ko tukdon mein todta hai, par alag tarike se (`CAPITALIZATION` -> `CA`, `PI`, `TAL`, `IZ`, `ATION`).\n",
     "# - Special characters/emojis ke liye yahan bhi � symbol aa raha hai (Unicode replacement character).\n",
     "# - Tabs bhi multiple space tokens mein toot rahe hain.\n",
     "# - Ismein SentencePiece ka alag style dikh raha hai tokenization ka. Start aur end mein </s> tokens bhi hain - yeh SentencePiece ke special tokens hain."
    ]
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yeh Code Kya Karta Hai? 🤔**\n",
    "\n",
    "Yeh code hamare test `text` ko Google ke Flan-T5 model ke tokenizer se process karta hai. Flan-T5 tokenizer GPT-2 jaisa hai ki yeh space ko word ka part treat karta hai। Socho yeh Flan-T5 ke translator se text ko tod kar dekh rahe hain। Yeh special characters/emojis ke liye `�` dikhata hai aur tabs ko multiple space tokens mein todta hai. Ismein SentencePiece tokenization use hoti hai, jo thodi alag hai, aur yeh sentence ke end mein `</s>` token add karta hai. Har tokenizer ka apna signature style hai!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: StarCoder2 Tokenizer Se Dekhna\n",
    "\n",
    "BigCode ka **StarCoder2** model code likhne aur samajhne ke liye train hua hai। Ab dekhte hain uska tokenizer hamare special test sentence ko kaise handle karta hai, jismein kuch programming keywords bhi hain। Yeh code ko kaise treat karta hai, yeh dekhna interesting hoga।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220
    },
    "executionInfo": null,
    "outputs": [],
    "source": [
     "# show_tokens function ko hamara text aur StarCoder2 tokenizer ka naam do.\n",
     "# \"bigcode/starcoder2-15b\" ek model hai jo khaas taur par code ke liye train hua hai.\n",
     "# Iska tokenizer bhi alag tarika use karta hai, jo code syntax ko samajhne mein helpful ho sakta hai.\n",
     "# Note: Kuch bade models ke liye Hugging Face website par access request karna pad sakta hai, par yeh chota version shayad easily available ho.\n",
     "show_tokens(text, \"bigcode/starcoder2-15b\")\n",
     "\n",
     "# Output dekho! Is tokenizer ka tarika thoda GPT-2/Flan-T5 jaisa lagta hai par code ke parts ko alag treat kar sakta hai:\n",
     "# - Spaces ko word ka part bana raha hai (tokens ke shuru mein space).\n",
     "# - Programming keywords (`False`, `None`, `elif`, `else`) aur operators (`==`, `>=`) ko alag tokens bana raha hai.\n",
     "# - CAPITALIZATION ko maintain rakhta hai.\n",
     "# - Emojis aur Chinese characters ke liye yahan bhi � symbol aa raha hai.\n",
     "# - Tabs ko yeh bhi space tokens mein tod raha hai.\n",
     "# Kyunki yeh code ke liye train hua hai, yeh code elements ko samajhne mein zyada accha ho sakta hai!"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yeh Code Kya Karta Hai? 🤔**\n",
    "\n",
    "Yeh code hamare test `text` ko BigCode ke StarCoder2 model ke tokenizer se process karta hai. Socho yeh StarCoder2 ke translator se text ko tod kar dekh rahe hain। Kyunki yeh model code par train hua hai, iska tokenizer programming related text (`False`, `None`, `==`, `>=`) ko alag tarike se treat kar sakta hai। Output mein dekho kaise yeh programming keywords aur operators ko tokens mein todta hai. Ismein bhi capital letters maintain rahte hain, emojis/special chars ke liye `�` hai, aur tabs spaces mein badalte hain। Yeh specialized tokenizers ka ek example hai।"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: Phi-3 Tokenizer Se Dekhna\n",
    "\n",
    "Ab wapas hamare pehle chapter wale dost, **Phi-3**, ka tokenizer dekhte hain। Humne isko pehle bhi dekha tha ki kaise yeh hamare prompt ko numbers mein badalta hai, par ab isko doosre tokenizers se compare karte hain using our `show_tokens` function।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": null,
    "outputs": [],
    "source": [
     "# show_tokens function ko hamara text aur Phi-3 tokenizer ka naam do.\n",
     "# \"microsoft/Phi-3-mini-4k-instruct\" hamara main AI dost hai jo humne shuru mein load kiya tha.\n",
     "# Iska tokenizer bhi SentencePiece jaisa technique use karta hai.\n",
     "show_tokens(text, \"microsoft/Phi-3-mini-4k-instruct\")\n",
     "\n",
     "# Output dekho! Phi-3 ka tokenizer bhi GPT-2 aur Flan-T5 jaisa hi kaam kar raha hai:\n",
     "# - Spaces ko word ka part bana raha hai (tokens ke shuru mein space).\n",
     "# - CAPITALIZATION ko maintain rakhta hai aur words ko tukdon mein todta hai, par tarika alag hai (`CAPITALIZATION` -> ` CAPITAL`, `IZATION`).\n",
     "# - Emojis aur Chinese characters ke liye yahan bhi � symbol aa raha hai.\n",
     "# - Tabs bhi spaces mein toot rahe hain.\n",
     "# - Numbers aur symbols (`12.0*50=600`) ko bhi tokens mein tod raha hai.\n",
     "# Yeh dikhata hai ki modern tokenizers mein common patterns hote hain, par har ek ki thodi alag 'personality' hoti hai text ko todne ki!"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yeh Code Kya Karta Hai? 🤔**\n",
    "\n",
    "Yeh code hamare test `text` ko Microsoft ke Phi-3 model ke tokenizer se process karta hai. Socho yeh hamare pehle AI dost ke translator se text ko tod kar dekh rahe hain। Iska tokenizer SentencePiece jaise tarike use karta hai. Output mein dekho ki yeh bhi spaces ko word ka part bana deta hai (jaise GPT-2 aur Flan-T5)। Capital letters maintain rahte hain, emojis/special characters ke liye `�` hai, aur tabs multiple space tokens mein badalte hain. Iska behavior dusre modern tokenizers se milta julta hai, lekin har ek mein thode variations hote hain। Yeh comparison humein batata hai ki AI words ko kaise dekhta hai!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Tu7OY4HvBEm"
   },
   "source": [
    "## 📊 Contextualized Word Embeddings (Meaning Numbers Jo Badalte Hain)\n",
    "\n",
    "Ab aata hai asli jaadu! ✨ AI models sirf text ko tokens mein nahi todte, balki woh har token ke liye special **meaning numbers** bhi banate hain। In numbers ko **Embeddings** kehte hain।\n",
    "\n",
    "Modern Language Models (jaise BERT, Phi-3, etc.) jo embeddings banate hain, woh **Contextualized** hote hain। Iska matlab hai ki ek hi shabd ka meaning number uske aas paas ke shabdon (sentence ke context) ke hisaab se badal jayega। Jaise \"bank\" shabd ka matlab \"river bank\" mein alag hoga aur \"money bank\" mein alag। Contextualized embeddings is difference ko numbers mein capture kar lete hain।\n",
    "\n",
    "Socho, har token ka embedding ek lambi list of numbers hai jo us token ka \"matlab\" describe karta hai us particular sentence mein। Jin tokens ka matlab similar hota hai, unki numbers ki lists bhi ek doosre ke \"paas\" hoti hain (jab hum un numbers ko ek bade space mein imagine karein)।\n",
    "\n",
    "Hum ek alag model load karenge jo embeddings banane ke liye accha hai (DeBERTa) aur dekhenge ki yeh kaise kaam karta hai।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": null,
    "outputs": [],
    "source": [
     "# embeddings ke liye special models hote hain. Hum DeBERTa naam ka model use karenge jo BERT jaisa hi hai par embeddings banane mein accha mana jata hai.\n",
     "# transformers library se AutoModel aur AutoTokenizer laate hain.\n",
     "# AutoModel ek general AI model hai jo embeddings (meaning numbers) de sakta hai.\n",
     "from transformers import AutoModel, AutoTokenizer\n",
     "\n",
     "# Ab ek tokenizer load karte hain jo DeBERTa model ke liye bana hai.\n",
     "# \"microsoft/deberta-base\" ek standard DeBERTa model ka naam hai Hugging Face par.\n",
     "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
     "\n",
     "# Ab Language Model (AI brain) load karte hain jo embeddings banayega.\n",
     "# Hum thoda chota version use kar rahe hain \"microsoft/deberta-v3-xsmall\" (tez chalega).\n",
     "# from_pretrained se model internet se download aur load karo.\n",
     "model = AutoModel.from_pretrained(\"microsoft/deberta-v3-xsmall\")\n",
     "\n",
     "# Ab ek simple sentence lete hain jiska hum embedding nikalenge.\n",
     "# Socho, agar sentence \"He is at the bank\" aur doosra \"The river bank is green\" hota, toh \"bank\" ke embeddings alag aate!\n",
     "sentence = 'Hello world'\n",
     "\n",
     "# Sentence ko tokenizer se numbers (tokens) mein badalte hain.\n",
     "# return_tensors='pt' matlab numbers ko PyTorch tensor (computer friendly number grid) format mein do - yeh format AI models samajhte hain.\n",
     "tokens = tokenizer(sentence, return_tensors='pt')\n",
     "\n",
     "# Ab in numbers (tokens) ko Super Engine (GPU) par bhej do.\n",
     "# {} ek dictionary hai. .items() se hum dictionary ke har key (k) aur value (v) par jaate hain.\n",
     "# v.to(\"cuda\") us value (jo ki numbers hain) ko GPU par move karta hai.\n",
     "# {k: v.to(\"cuda\") for k, v in tokens.items()} yeh poori dictionary ke numbers ko GPU par bhej deta hai.\n",
     "# Agar GPU nahi hai toh '.to(\"cuda\")' hata dena ya '.to(\"cpu\")' kar dena.\n",
     "tokens = {k: v.to(\"cuda\") for k, v in tokens.items()}\n",
     "\n",
     "# Ab in numbers (tokens) ko model ke andar se process karwate hain aur usse output lete hain.\n",
     "# model(**tokens) matlab model ko tokens as input do. '**' ka matlab hai ki 'tokens' dictionary ke har item ko ek alag input ki tarah model ko bhejo.\n",
     "# [0] se model ka pehla output lete hain. DeBERTa jaise models mein pehla output hi tokens ke Contextualized Embeddings hote hain.\n",
     "output = model(**tokens)[0]\n",
     "\n",
     "# Ab output variable mein hamare sentence ke embeddings (meaning numbers) store hain!\n",
     "# Har token ke liye alag alag numbers ki list hai, jo us token ka matlab bata rahi hai us sentence ke context mein। Yahi hain Contextualized Embeddings."
    ]
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yeh Code Kya Karta Hai? 🤔**\n",
    "\n",
    "Yeh code `transformers` library use karke ek tokenizer aur ek DeBERTa naam ka Language Model load karta hai. DeBERTa BERT jaisa hi ek model hai jo text ko samajh kar har token ke liye **Contextualized Embeddings** (meaning numbers) banata hai। Socho yeh ek model hai jo har shabd ka matlab sentence ke hisaab se adjust karta hai। Hum ek sentence (\"Hello world\") ko `tokenizer` use karke numbers mein badalte hain (`tokens`), aur phir un numbers ko `model` mein daal kar usse **embeddings** nikalte hain। `output` variable mein yahi embeddings store hote hain। `device_map=\"cuda\"` aur `.to(\"cuda\")` se hum ensure karte hain ki yeh sab Super Engine (GPU) par ho, taaki tez chale।"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: Embedding Ke Numbers Ka Shape Dekhna\n",
    "\n",
    "Embeddings sirf ek number nahi hota, balki numbers ki puri list hoti hai (jisko technical language mein **vector** kehte hain). Socho yeh har token ka \"matlab score card\" hai jismein kai sare numbers hain। Is list ki lambai ko **embedding size** kehte hain। Dekhte hain hamare `output` ka shape kya hai। Isse pata chalega kitne tokens hain aur har token ka vector kitna lamba hai।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": null,
    "outputs": [],
    "source": [
     "# Hamare output (embeddings) ka shape (dimensions) dekhte hain.\n",
     "# .shape property se tensor (number grid) ka shape pata chalta hai.\n",
     "output.shape\n",
     "\n",
     "# Output mein dikhega \"torch.Size([1, 4, 384])\" (numbers thode alag ho sakte hain depending on the model).\n",
     "# Iska matlab hai:\n",
     "# [1,...]: Yeh dikhata hai ki humne 1 sentence diya hai (isko \"Batch size\" kehte hain - kitne sentences ek saath process kiye).\n",
     "# [..., 4,...]: Hamare sentence mein 4 tokens the (jitne tokenizer ne banaye the).\n",
     "# [..., 384]: Har token ke liye 384 numbers ki list mili hai (iski \"Embedding size\" ya \"vector dimension\" 384 hai).\n",
     "# So, humare paas 1 sentence ke 4 tokens ke liye, har token ka 384 numbers ka contextualized embedding vector hai."
    ]
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yeh Code Kya Karta Hai? 🤔**\n",
    "\n",
    "Yeh code hamare `output` embeddings variable ka `shape` print karta hai। Shape dikhata hai ki embeddings kaise organize hain। `torch.Size([1, 4, 384])` (numbers vary ho sakte hain) ka matlab hai: Humne 1 sentence process kiya hai (`1`), usmein 4 tokens nikle hain (`4`), aur har token 384 numbers ki list (vector) se represent ho raha hai (`384`)। Yeh 384 **Embedding Size** hai। Socho, har token ka ek score card hai jismein 384 scores hain! Kyunki yeh Contextualized Embedding hai, har token ka apna alag vector hai, jo sentence mein uske matlab ko capture karta hai।"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: Embedding Model Ke Tokens Dekhna\n",
    "\n",
    "Jis sentence ka humne embedding nikala hai (\"Hello world\"), uske tokens dekhte hain। Taki hum embeddings ke shape (`[1, 4, 384]`) se tokens (`4`) ko match kar sakein aur confirm karein ki 4 tokens kaunse the jinke 4 vectors bane hain।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": null,
    "outputs": [],
    "source": [
     "# Jis sentence ke liye embeddings nikale hain ('Hello world'), uske tokens dekhte hain using wapas tokenizer.decode().\n",
     "# tokens['input_ids'][0] mein woh numbers (token IDs) hain jo tokenizer ne banaye the 'Hello world' ke liye.\n",
     "# for token in tokens['input_ids'][0]: is list mein har number (token ID) par jao ek-ek karke.\n",
     "# print(tokenizer.decode(token)): Har number ko wapas text mein badlo aur screen par dikhao.\n",
     "for token in tokens['input_ids'][0]:\n",
     "    print(tokenizer.decode(token))\n",
     "\n",
     "# Output mein dekho: [CLS], Hello, world, [SEP].\n",
     "# Total 4 tokens hain, jo hamare embedding shape [1, 4, 384] ke beech wale number (4) se match karta hai.\n",
     "# DeBERTa tokenizer ne \" Hello world\" ko teen tokens mein toda hai (\"Hello\", \" world\") aur shuru/end mein special tokens add kiye hain ([CLS], [SEP])."
    ]
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yeh Code Kya Karta Hai? 🤔**\n",
    "\n",
    "Yeh code dikhata hai ki DeBERTa tokenizer ne \"Hello world\" sentence ko kin tokens mein toda tha: `[CLS]`, `Hello`, ` world`, aur `[SEP]`. Total 4 tokens hain, jismein space bhi ek token का हिस्सा है (` world`). Socho yeh woh 4 LEGO pieces hain jo \"Hello world\" sentence ko represent karte hain AI ke andar। Yeh number (4) hamare embeddings ke shape ke beech wale number se match karta hai, jo confirm karta hai ki har token ke liye ek embedding vector bana hai। `[CLS]` aur `[SEP]` special tokens hain jo BERT jaise models sentence ki boundary batane ke liye use karte hain।"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: Raw Embedding Numbers Dekhna\n",
    "\n",
    "Ab embeddings ke actual numbers dekhte hain। Yeh woh lambi lists of numbers hain jo AI model ne har token ke liye banaye hain। Yeh number lists hain jo har token ka context-specific \"matlab\" capture karti hain।\n",
    "\n",
    "💡 **Picture Suggestion:** Yahan par ek matrix (grid of numbers) dikha sakte ho, jismein rows tokens hain ([CLS], Hello,  world, [SEP]) aur columns unke embedding numbers। Har row ek lambi list of numbers hai।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": null,
    "outputs": [],
    "source": [
     "# Hamare output (embeddings) ke actual numbers dekhte hain.\n",
     "# output variable ko print karne se uske andar ke numbers (tensor ka content) dikh jaate hain.\n",
     "# Yeh har token ke liye 384 numbers ki list hai.\n",
     "output\n",
     "\n",
     "# Output mein bahut saare decimal numbers ki list dikhegi.\n",
     "# Pehli list ([...]), [CLS] token ke numbers hain.\n",
     "# Doosri list ([...]), \"Hello\" token ke numbers hain.\n",
     "# Teesri list ([...]), \" world\" token ke numbers hain.\n",
     "# Aur chauthi list ([...]), [SEP] token ke numbers hain.\n",
     "# Yahi numbers AI model use karta hai aage ki processing ke liye aur yeh Numbers hi text ka meaning capture karte hain!"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yeh Code Kya Karta Hai? 🤔**\n",
    "\n",
    "Yeh code `output` variable ka content print karta hai, jo ki hamare tokens ke liye bane hue **Contextualized Embeddings** ka tensor (number grid) hai। Output mein bahut saare decimal numbers ki list dikhegi। Har row (jaise `[-3.4816, 0.0861, ...]`) ek token ka 384-dimensional vector hai। Socho yeh har LEGO piece ka complex \"matlab score card\" hai! Yahi numbers AI ke andar math karne aur baatein samajhne ke liye use hote hain।"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DdEDuLWa0r4L"
   },
   "source": [
    "## 📄 Text Embeddings (Poore Sentence Ya Document Ke Liye Meaning Numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "executionInfo": null,
    "outputs": [],
    "source": [
     "# embeddings ko poore sentence ya document ke liye bhi bana sakte hain. Inhein Text Embeddings kehte hain.\n",
     "# sentence-transformers library aise models load karna aur use karna aasan banati hai.\n",
     "from sentence_transformers import SentenceTransformer\n",
     "\n",
     "# Ab ek Sentence Embedding model load karte hain.\n",
     "# 'sentence-transformers/all-mpnet-base-v2' ek popular model hai jo specially sentences ko vectors mein badalne ke liye train hua hai.\n",
     "# Isko load karne mein thoda time aur internet use hoga kyunki yeh model download hota hai.\n",
     "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
     "\n",
     "# Ab ek sentence lete hain jiska text embedding nikalna hai.\n",
     "sentence = \"Best movie ever!\"\n",
     "\n",
     "# Sentence ko model ke .encode() method se seedha text embedding (numbers ki list) mein badalte hain.\n",
     "# Yeh .encode() method internally tokenizer use karta hai aur phir model se poore sentence ka ek single vector nikalta hai.\n",
     "# Yeh bahut convenient hai poore sentence ka ek 'meaning number' chahiye ho toh.\n",
     "vector = model.encode(sentence)\n",
     "\n",
     "# Ab vector variable mein hamare sentence ka text embedding store hai!\n",
     "# Yeh poore sentence ka ek single \"meaning number\" vector hai, na ki har token ka alag alag."
    ]
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yeh Code Kya Karta Hai? 🤔**\n",
    "\n",
    "Yeh code `sentence-transformers` library use karke ek Sentence-BERT model load karta hai। Socho yeh ek special translator hai jo poore sentence ko ek hi meaning number list mein badal deta hai। Yeh model specially design kiya gaya hai **Text Embeddings** ya **Sentence Embeddings** banane ke liye. Hum ek sentence (`\"Best movie ever!\"`) ko `model.encode()` function mein daal kar uska ek single numerical vector (`vector`) nikalte hain। Yeh vector poore sentence ke matlab ko represent karta hai। Yeh aksar tab use hota hai jab hum sentences ya documents ke beech similarity dhundhna chahte hain (jin sentences ke numbers paas hain, woh meaning mein similar honge)।"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: Sentence Embedding Ke Numbers Ka Shape Dekhna\n",
    "\n",
    "Dekhte hain ki Text Embedding ka shape kaisa hota hai. Kyunki yeh poore sentence ka single meaning number hai, toh hamein ek hi vector milna chahiye। Compare karo iske shape ko Contextualized Embeddings ke shape se।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": null,
    "outputs": [],
    "source": [
     "# Hamare sentence embedding (vector) ka shape dekhte hain.\n",
     "# .shape property se is number list ki lambai pata chalegi.\n",
     "vector.shape\n",
     "\n",
     "# Output mein dikhega \"(768,)\" (number alag ho sakta hai depending on the model).\n",
     "# Iska matlab hai ki hamara poora sentence \"Best movie ever!\" ek single list of 768 numbers se represent ho raha hai.\n",
     "# Yahan koi alag token ki dimension nahi hai, jaise Contextualized embeddings mein thi (jahan [1, 4, 384] shape tha - 4 tokens, har token 384 numbers).\n",
     "# Yeh 768 is model ka embedding size hai. Yahi hai poore sentence ka ek vector."
    ]
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yeh Code Kya Karta Hai? 🤔**\n",
    "\n",
    "Yeh code hamare `vector` variable ka `shape` print karta hai। Output `(768,)` (number vary ho sakta hai) dikhayega. Iska matlab hai ki hamara poora sentence ek single list of 768 numbers se represent ho raha hai। Socho yeh poore LEGO model ka ek final score hai! Contextualized embeddings se alag, yahan har token ka alag vector nahi hai, balki poore sentence ka ek consolidated vector hai। Yahi hain **Text Embeddings**। In vectors ka use karke hum do sentences ya documents ki similarity measure kar sakte hain (jitne paas vectors, utna similar matlab)।"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnuGRjo80yKj"
   },
   "source": [
    "## 🕰️ Word Embeddings Jo LLMs Se Purane Hain (Non-Contextual)\n",
    "\n",
    "Modern Language Models (LLMs) jaise BERT aur Phi-3 naye hain AI ki duniya mein। Inke aane se pehle bhi words ko numbers mein badalne ke tarike the। Unhein simply **Word Embeddings** kehte the। Do bahut popular purane tarike the **Word2Vec** aur **GloVe**।\n",
    "\n",
    "In purane embeddings mein ek khas baat thi: ek shabd ka meaning number hamesha same rehta tha, bhale hi woh kisi bhi sentence mein aaye। Socho jaise purani dictionary jismein har shabd ka ek hi matlab likha ho। Isliye inhein **Non-Contextual Embeddings** kehte hain।\n",
    "\n",
    "Yeh Contextualized embeddings se alag hai, jahan \"bank\" ka matlab \"river bank\" mein alag vector deta hai aur \"money bank\" mein alag। Non-Contextual mein dono baar \"bank\" ka vector same rahega।\n",
    "\n",
    "Chalo ek purana model (GloVe) load karke dekhte hain ki yeh kaise kaam karta tha।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": null,
    "outputs": [],
    "source": [
     "# AI ki duniya mein Language Models (LLMs) naye hain. LLMs se pehle bhi words ko numbers mein badalne ke tarike the.\n",
     "# Unhein simply Word Embeddings kehte the (jaise Word2Vec, GloVe).\n",
     "# In purane embeddings mein ek shabd ka meaning number hamesha same rehta tha, bhale hi woh kisi bhi sentence mein aaye (Non-Contextual).\n",
     "\n",
     "# gensim library se downloader tool laate hain. Yeh library purane embedding models ke saath kaam karne ke liye famous hai.\n",
     "import gensim.downloader as api\n",
     "\n",
     "# Ab ek pre-trained embedding model download karke load karte hain.\n",
     "# api.load() se hum gensim ke data store se model download kar sakte hain.\n",
     "# \"glove-wiki-gigaword-50\": Yeh model ka naam hai - GloVe naam ki technique se bana hai, Wikipedia par train hua, har word ke liye 50 numbers (vector size: 50).\n",
     "# Yeh file download hone mein thoda time lag sakta hai (approx 66MB). Yeh sirf ek baar download hoga. Socho ek purani dictionary download ho rahi hai numbers wali.\n",
     "model = api.load(\"glove-wiki-gigaword-50\")\n",
     "\n",
     "# Ab hamara GloVe embedding model load ho gaya hai!\n",
     "# Yeh hazaron common English shabdon aur unke corresponding 50 numbers wale vectors ko janta hai."
    ]
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yeh Code Kya Karta Hai? 🤔**\n",
    "\n",
    "Yeh code `gensim` library ka use karke ek **Non-Contextual Word Embedding** model download aur load karta hai jiska naam **GloVe** hai. Socho yeh ek purani dictionary hai jismein har shabd ke aage uske matlab ko batane wale numbers ki list hai, aur woh numbers kabhi badalte nahi। Isliye isko Non-Contextual kehte hain। Yeh model Wikipedia ke bade text data par train hua hai aur ismein common English words aur unke corresponding 50-dimensional embedding vectors store hain। Yeh embeddings Contextualized embeddings se alag hain kyunki yahan \"bank\" (river) aur \"bank\" (money) ka vector (meaning number list) same hoga।"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: GloVe Se Similar Words Dhundhna\n",
    "\n",
    "In embeddings ka ek cool feature hai ki jin shabdon ka matlab similar hota hai, unke meaning numbers bhi ek doosre ke **paas** hote hain multi-dimensional space mein। Socho agar words ko points bana dein ek bade se graph par, toh similar matlab wale words ke points ek doosre ke close honge। Isliye hum `most_similar` function use karke kisi word ke sabse similar words dhundh sakte hain meaning ke hisaab se!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": null,
    "outputs": [],
    "source": [
     "# Ab load kiye hue GloVe model se \"king\" word ke sabse similar words dhundhte hain.\n",
     "# model['king']: \"king\" word ka 50 numbers wala vector (embedding) nikalte hain.\n",
     "# model.most_similar(): Model ko ek vector do aur woh uske sabse paas wale word vectors dhundh kar unke words bata dega.\n",
     "# positive=[model['king']]: Humein \"king\" ke vector ke similar words chahiye. 'positive' is liye kyunki hum similar words add kar rahe hain concept mein.\n",
     "# topn=11: Sabse similar 11 words batao. Hum 11 mang rahe hain kyunki 'king' khud bhi similar list mein aata hai (uski similarity khud se 1 hoti hai), isliye 11 mang rahe hain taaki king ko chhod kar top 10 milen.\n",
     "model.most_similar([model['king']], topn=11)\n",
     "\n",
     "# Output mein dekho! \"king\" ke sabse similar words \"prince\", \"queen\", \"emperor\" jaise aa rahe hain.\n",
     "# Yeh dikhata hai ki in numbers ne shabdon ke matlab ko kitne acchi tarah se capture kiya hai.\n",
     "# Similarity score bhi hai (second number in each tuple), jitna 1 ke paas, utna zyada similar matlab hai GloVe ke hisaab se."
    ]
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yeh Code Kya Karta Hai? 🤔**\n",
    "\n",
    "Yeh code GloVe model ke `most_similar` function ka use karke \"king\" word ke embedding vector ke sabse paas (meaning mein similar) words dhundhta hai। Socho yeh us bade graph par \"king\" ke point ke aas paas wale points ko dhundh raha hai। Output mein aise words aate hain jo meaning ke hisaab se \"king\" se close hain (jaise \"prince\", \"queen\", \"emperor\"). Ismein similarity score bhi dikhta hai jo batata hai ki kaunsa word kitna similar hai। Yeh dikhata hai ki kaise embeddings shabdik rishton (semantic relationships) ko numerical space mein capture kar sakte hain, bhale hi yeh purane Non-Contextual tarike se banaye gaye hon।"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMSgyKKS4xUx"
   },
   "source": [
    "## 🎵 Embedding Se Song Recommend Karna (Ek Real-Life Example!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3dJdWzT67nDL"
   },
   "outputs": [],
   "source": [
    "# Embeddings ka ek real-life use case dekhte hain - gaane recommend karna! 🎶\n",
    "# Jaise words similar hote hain meaning mein, waise hi hum gaano (songs) ko bhi numbers mein badal sakte hain aur phir similar gaane dhundh sakte hain.\n",
    "# Ek simple tarika hai yeh dekhna ki kaunse gaane log aksar ek saath sunte hain playlists mein. Jo gaane ek saath aate hain, woh shayad meaning mein similar honge (jaise ek type ke honge - rock, pop, sad, happy)। Socho playlist ek sentence hai aur har song ek shabd!\n",
    "\n",
    "# Hum ek dataset use karenge jismein user playlists hain (sirf song IDs ki list), aur phir uspar ek Word2Vec jaisa model train karenge.\n",
    "# Model har song ID ko ek meaning number (embedding) dega, aur phir hum similar numbers wale songs dhundh kar recommend karenge.\n",
    "\n",
    "# Zaroori tools laate hain: pandas (data tables ke liye) aur urllib (online file download ke liye).\n",
    "import pandas as pd\n",
    "from urllib import request\n",
    "\n",
    "# Online se playlist dataset file download karte hain. Yeh file mein user ki banayi hui playlists hain, har playlist mein song IDs ki list hai.\n",
    "# request.urlopen() internet se file open karta hai.\n",
    "data = request.urlopen('https://storage.googleapis.com/maps-premium/dataset/yes_complete/train.txt')\n",
    "\n",
    "# Downloaded file ko read karte hain, bytes se text mein badalte hain (.decode(\"utf-8\")), lines mein todte hain (.split('\\n')).\n",
    "# Shuru ki do lines mein extra info hai (metadata), unko chhod dete hain [2:] se.\n",
    "lines = data.read().decode(\"utf-8\").split('\\n')[2:]\n",
    "\n",
    "# Har line (playlist) ko chote numbers (song IDs) mein todte hain.\n",
    "# s.rstrip().split() line ke end se extra space hatata hai aur words (IDs) mein todta hai.\n",
    "# List comprehension [...] for ... if ... use karke hum sirf woh playlists rakhte hain jismein 1 se zyada songs hain (len(s.split()) > 1) - kyunki single song playlist mein meaning nahi seekh sakte.\n",
    "playlists = [s.rstrip().split() for s in lines if len(s.split()) > 1]\n",
    "\n",
    "# Ab song ki details (naam, artist) wali file load karte hain.\n",
    "# Online se file download karo, read karo, decode karo, lines mein todo.\n",
    "songs_file = request.urlopen('https://storage.googleapis.com/maps-premium/dataset/yes_complete/song_hash.txt')\n",
    "songs_file = songs_file.read().decode(\"utf-8\").split('\\n')\n",
    "# Har line ko song ID, title, artist mein todo (split('\\t') tab character par todta hai).\n",
    "songs = [s.rstrip().split('\\t') for s in songs_file]\n",
    "# pandas DataFrame mein daalo. DataFrame ek smart table jaisa hai Python mein.\n",
    "# columns = ['id', 'title', 'artist'] un columns ke naam set karte hain.\n",
    "songs_df = pd.DataFrame(data=songs, columns = ['id', 'title', 'artist'])\n",
    "# Song ID column ko index bana do taaki ID se details quickly dhundh sakein. Index row number jaisa hota hai jisse search tez hota hai.\n",
    "songs_df = songs_df.set_index('id')\n",
    "\n",
    "# Data load ho gaya! Playlists list mein hain aur song details songs_df (DataFrame) mein.\n",
    "# Ab hum is data ko use karke recommendations system banayenge!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yeh Code Kya Karta Hai? 🤔**\n",
    "\n",
    "Yeh code ek online dataset download karta hai jismein user playlists (`playlists`) aur song details (`songs_df`) hain. Socho yeh tumhare music app ka data download kar raha hai! Playlists mein sirf song IDs ki lists hain, aur song details mein har ID ke liye song ka naam aur artist hai. Hum is data ka use karke seekhenge ki kaise songs ko embeddings mein badal kar recommendations banate hain। `pandas` data organize karne mein help karta hai aur `urllib.request` files download karne mein। Humne playlists ko aise process kiya ki har playlist ek \"sentence\" hai aur har song ID ek \"word\"।"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: Sample Playlists Dekhna\n",
    "\n",
    "Load kiye hue playlists kaise dikhte hain, chalo dekhte hain। In lists mein sirf song IDs hain।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": null,
    "outputs": [],
    "source": [
     "# Load kiye hue playlists mein se pehle 2 playlists dekhte hain.\n",
     "# playlists ek Python list hai (jaise saman ki list), iske items ko index [0], [1] se access karte hain.\n",
     "# '\\n' new line ke liye hai, taaki output saaf dikhe.\n",
     "# print() function screen par output dikhata hai.\n",
     "print( 'Playlist #1:\\n ', playlists[0], '\\n')\n",
     "print( 'Playlist #2:\\n ', playlists[1])\n",
     "\n",
     "# Output mein dekho, yeh numbers (song IDs) ki lambi lambi lists hain.\n",
     "# Har list ek user ki playlist hai.\n",
     "# Model in lists ko padh kar seekhega ki kaunse song IDs (songs) aksar ek saath aate hain.\n",
     "# Socho jaise sentences jismein 'words' song IDs hain."
    ]
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yeh Code Kya Karta Hai? 🤔**\n",
    "\n",
    "Yeh code `playlists` list ke pehle do items print karta hai. Yeh dikhata hai ki playlist data kaisa dikhta hai - song IDs ki lists. Har list ek user ki playlist ko represent karti hai. Yahi sequences hain jinpar hum apna song embedding model train karenge। Socho yeh hamare model ke liye training sentences hain। Jo songs ek hi playlist mein ya ek doosre ke aas paas aate hain, Word2Vec unhein similar maanega aur unke meaning numbers (embeddings) ko numerical space mein paas laayega।"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: Playlists Par Naya Embedding Model (Word2Vec) Train Karna\n",
    "\n",
    "Ab hum playlists ke data par **Word2Vec** naam ka ek embedding model train karenge. Word2Vec ek purana Non-Contextual embedding algorithm hai jo shabdon ke aas paas ke shabdon ko dekh kar unka meaning seekhta hai। Yahan hamare \"shabd\" song IDs hain aur \"sentence\" playlists hain। Yeh model playlists ko padhega aur har song ID ke liye ek \"meaning number\" (embedding) banayega। Jo songs ek hi playlist mein ya aas paas aate hain, unke embeddings similar banenge।\n",
    "\n",
    "Socho, jaise \"cat\" aur \"dog\" aksar ek saath dikhte hain text mein, Word2Vec unke vectors ko paas layega। Waise hi, jo gaane ek playlist mein hain, unke vectors paas ho jayenge।\n",
    "\n",
    "💡 **Picture Suggestion:** Yahan ek image dikha sakte ho jismein 'playlist 1: songA, songB, songC', 'playlist 2: songB, songD, songE' jaisa kuch likha ho, aur side mein ek graph ho jismein points (songA, songB, songC, songD, songE) bane hon aur songA, songB, songC paas paas hon aur songB, songD, songE paas paas hon, dikhate hue ki songB common hai aur uske dost badalte hain, par is model mein songB ka vector same hi rahega bhale hi uske alag dost hon, bas uske doston ke vector uske paas banenge।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EaUz3E0P7sJs"
   },
   "outputs": [],
   "source": [
    "# gensim.models se Word2Vec tool laate hain. Yeh woh algorithm hai jo embeddings banayega playlist data se.\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Ab hamara Word2Vec model train karte hain.\n",
    "# model = Word2Vec(...) se model banate hain aur train karna shuru karte hain.\n",
    "# playlists: Yeh hamara data hai (lists of song IDs), jisko model padhega (jaise text corpus Word2Vec ke liye).\n",
    "# vector_size=32: Har song ID ke liye 32 numbers ka vector banega (embedding size 32). yeh numbers ki list ki lambai hai.\n",
    "# window=20: Model dekhega ki ek song ke aas paas playlist mein 20 songs tak kaunse songs aate hain (context window). Isse woh seekhega ki kaunse songs aksar ek saath sunte hain.\n",
    "# negative=50: Training ke liye ek advanced technique (Negative Sampling) jo model ko improve karti hai. Ismein model galat predictions ko punish karta hai.\n",
    "# min_count=1: Jo song ID kam se kam 1 baar playlist mein aaya hai, uska embedding banega. Bahut kam dikhne wale songs ko ignore nahi kar rahe.\n",
    "# workers=4: Agar tumhare computer mein 4 CPU cores hain, toh training mein 4 cores use karo (agar available hon) taaki tez ho. Socho 4 log ek saath kaam kar rahe hain.\n",
    "model = Word2Vec(\n",
    "    playlists, vector_size=32, window=20, negative=50, min_count=1, workers=4\n",
    ")\n",
    "\n",
    "# Model train ho gaya! 🎉\n",
    "# Ab har song ID ke liye hamare paas ek 32 numbers ka embedding hai jo playlists ke data se seekha gaya hai.\n",
    "# Yeh embeddings playlists mein songs ke saath aane ki 'dosti' ko numbers mein capture karte hain।"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yeh Code Kya Karta Hai? 🤔**\n",
    "\n",
    "Yeh code `gensim` library ka Word2Vec algorithm use karke hamare `playlists` data par ek naya embedding model train karta hai. Socho yeh model users ke music taste ko numbers mein badal raha hai। Word2Vec har unique song ID ko ek 32-dimensional numerical vector (embedding) assign karta hai। Training ka goal hota hai ki jo song IDs (songs) playlists mein aksar ek doosre ke paas aate hain (jaise \"song A\" ke baad \"song B\"), unke vectors bhi numerical space mein ek doosre ke paas hon। Is tarah, embeddings songs ki 'similarity' ya 'relatedness' ko playlists ke behavior se capture karte hain।"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: Train Kiye Model Se Similar Song IDs Dhundhna\n",
    "\n",
    "Ab jo model train hua hai, usse hum kisi song ID ke sabse similar song IDs pooch sakte hain। Yeh similar IDs woh hongi jinke embeddings numerical space mein given song ID ke embedding ke sabse paas hon। Socho yeh numerical space mein jaakar given song ke point ke sabse close points dhundh raha hai। Yeh step recommendation system ka core hai।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": null,
    "outputs": [],
    "source": [
     "# Ek song ID choose karte hain recommendations dekhne ke liye.\n",
     "# Hum song ID 2172 choose karte hain. Baad mein hum iski details dekhenge ki yeh kaunsa gaana hai.\n",
     "song_id = 2172\n",
     "\n",
     "# Model se poochhte hain ki song ID 2172 ke sabse similar 10 song IDs kaunsi hain.\n",
     "# model.wv: Yeh trained model ke word vectors (embeddings) ka access deta hai. wv matlab \"word vectors\".\n",
     "# .most_similar(positive=str(song_id)): song_id (jisko string format mein badal diya hai) ke vector ke similar vectors dhundho.\n",
     "#                                      'positive' parameter mein hum woh item (song ID) dete hain jiske jaisa hamein dhundhna hai.\n",
     "# topn=10: Sabse similar 10 vectors aur unke corresponding song IDs batao.\n",
     "# Gensim ka most_similar result mein original item ko shamil nahi karta (unlike GloVe example upar).\n",
     "model.wv.most_similar(positive=str(song_id))"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yeh Code Kya Karta Hai? 🤔**\n",
    "\n",
    "Yeh code hamare train kiye hue Word2Vec `model` ka `most_similar` function use karke choose kiye hue `song_id` (2172) ke embedding vector ke numerical space mein sabse paas wale 10 song ID vectors dhundhta hai। `positive=str(song_id)` specify karta hai ki hamein is song ke similar songs chahiye. Output mein un song IDs ki list aati hai, saath mein similarity score bhi, jo Word2Vec ke hisaab se given song ke similar hain (kyunki woh playlists mein aksar saath dikhte the)। Yahi hai embeddings ka power - recommendations banane mein use hona! Socho, yeh numerical space mein chhupa hua music taste dhundh raha hai।"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: Original Song Ke Details Dekhna\n",
    "\n",
    "Song ID 2172 kya tha, yeh yaad nahi hai? Chalo uski details dekhte hain hamare `songs_df` (DataFrame) se। Socho yeh song ID ka code use karke music library mein gaane ka naam dhundh rahe hain। Yeh step confirm karne ke liye hai ki recommendation sahi context mein hai।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": null,
    "outputs": [],
    "source": [
     "# song ID 2172 ki details (title aur artist) dekhte hain songs_df se.\n",
     "# songs_df hamari songs ki smart table hai jismein index song ID hai.\n",
     "# .loc[] se hum index (song ID) use karke row nikal sakte hain.\n",
     "# Hum 2172 ko string format mein de rahe hain (\"2172\") kyunki DataFrame ka index column jo song IDs se bana hai, woh string data type ka hai.\n",
     "# print() se screen par dikhao details.\n",
     "print(songs_df.loc[str(2172)]) # .loc[] string index ke saath accha kaam karta hai.\n",
     "\n",
     "# Output mein dekho, yeh \"Fade To Black\" by Metallica tha. Ab recommendations samajhna aasan hoga!\n",
     "# Iski recommendations Metal ya Rock songs honi chahiye!"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yeh Code Kya Karta Hai? 🤔**\n",
    "\n",
    "Yeh code `songs_df` DataFrame (hamari song details ki table) ka use karke `song_id` 2172 ke details (title aur artist) print karta hai. Socho tumne song ID ka code enter kiya aur table ne gaane ka naam aur artist bata diya। Kyunki humne `song_hash.txt` file ko load karte waqt song ID ko index banaya tha, hum `songs_df.loc[str(2172)]` se us song ki row easily nikal sakte hain। Isse humein pata chalta hai ki hum kis original song ke liye recommendations dekh rahe hain। Recommendations check karte waqt original item ki details pata hona accha hai!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: Recommendations Dikhane Wala Function Banana\n",
    "\n",
    "Ab recommendations dhundhne (using Word2Vec) aur unki details dikhane (using DataFrame) ke steps ko ek function mein daalte hain। Function banane se code clean, organized aur reusable ho jata hai। Yeh function kisi bhi song ID ke liye recommendations dhundh kar unki details dikhayega।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "executionInfo": null,
    "outputs": [],
    "source": [
     "# import numpy library for some math help (hum iska use recommended IDs ko list se array mein badalne ke liye karenge).\n",
     "import numpy as np\n",
     "\n",
     "# Yeh function kisi bhi song ID ke liye recommendations dhundh kar unki details dikhayega.\n",
     "# def print_recommendations(song_id): matlab function ka naam print_recommendations hai aur woh ek input song_id lega.\n",
     "def print_recommendations(song_id):\n",
     "    # model.wv.most_similar se similar songs ke IDs aur scores milte hain.\n",
     "    # positive=str(song_id): hamare input song ID ko string mein badal kar model ko do (Word2Vec string IDs se kaam karta hai).\n",
     "    # topn=5 matlab top 5 similar songs chahiye.\n",
     "    similar_songs_with_scores = model.wv.most_similar(positive=str(song_id), topn=5)\n",
     "\n",
     "    # similar_songs_with_scores ek list of tuples hai, jaise [('2849', 0.99), ('2640', 0.99), ...]\n",
     "    # Ab humein sirf song IDs chahiye (pehla item in each tuple).\n",
     "    # list comprehension [song_id for song_id, score in similar_songs_with_scores] yeh kaam karta hai - yeh list ke har tuple mein se pehla item nikal kar ek nayi list banata hai.\n",
     "    similar_song_ids = [song_id for song_id, score in similar_songs_with_scores]\n",
     "\n",
     "    # Optional: Original song details print karo taaki pata chale ki kiske liye recommendations dekh rahe hain.\n",
     "    print(songs_df.loc[str(song_id)]) # Original song detail nikalne ke liye DataFrame ka .loc use karo string ID ke saath.\n",
     "    # Optional: Sirf recommended IDs ki list print karo agar dekhna ho numbers kya hain.\n",
     "    print(similar_song_ids) # Just the list of IDs\n",
     "\n",
     "    # Ab songs_df.loc[] use karke un similar song IDs ki details (title, artist) nikalte hain.\n",
     "    # songs_df ka index song ID (string) hai, so .loc[] works best here.\n",
     "    # similar_song_ids ek list of strings hai jisko hum DataFrame ke index mein lookup kar rahe hain.\n",
     "    return  songs_df.loc[similar_song_ids] # Un IDs ki rows (title, artist) DataFrame se nikal kar wapas do function se.\n",
     "\n",
     "# Function taiyaar hai! Ab isko use kar sakte hain kisi bhi song ID ke liye recommendations lene ke liye.\n",
     "\n",
     "# Example: Song ID 2172 (Fade To Black by Metallica) ke recommendations dekhte hain.\n",
     "# print_recommendations(2172) ko call karo aur function ka output (DataFrame) screen par dikh jayga.\n",
     "print_recommendations(2172)\n",
     "\n",
     "# Output mein ek table (DataFrame) dikhega recommended songs ka titles aur artists ke saath."
    ]
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yeh Code Kya Karta Hai? 🤔**\n",
    "\n",
    "Yeh code `print_recommendations` naam ka ek function banata hai। Socho yeh ek music recommender machine hai! Yeh function ek song ID input leta hai। Us ID ko string mein badal kar `model.wv.most_similar` se top 5 similar song IDs aur unke similarity scores dhundhta hai। Phir similar song IDs ki list extract karta hai। Iske baad, `songs_df.loc[]` ka use karke un 5 recommended song IDs ki details (title, artist) nikal kar ek DataFrame mein wapas karta hai। Yeh function hamara recommendation logic hai।\n",
    "\n",
    "Jab hum `print_recommendations(2172)` call karte hain, toh yeh function song 2172 ke liye top 5 recommendations dhundh kar ek DataFrame mein dikhata hai. Pichhle step mein humne dekha tha ki song 2172 Metallica ka \"Fade To Black\" hai। Ab dekhte hain iske recommendations kya aate hain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "executionInfo": null,
    "outputs": [],
    "source": [
     "# print_recommendations function ko song ID 842 do aur uska output print karo.\n",
     "# Dekhte hain Hip-Hop song ke liye kya recommendations aate hain.\n",
     "print_recommendations(842)\n",
     "\n",
     "# Output mein dekho! Ab recommendations Hip-Hop ke context mein aa rahe hain.\n",
     "# Jaise \"How We Do\" (The Game), \"If I Ruled The World\" (Nas), \"Heartless\" (Kanye West).\n",
     "# Yeh dikhata hai ki embeddings sirf music style hi nahi, balki sub-genres aur related artists/songs ko bhi capture kar sakte hain!\n",
     "# Yahi hai embeddings ka jaadu - shabdon ya items ke matlab ko numbers mein badal kar unke beech ke rishte samajhna."
    ]
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yeh Code Kya Karta Hai? 🤔**\n",
    "\n",
    "Yeh code pehle song ID 842 ke details print karta hai (jo ki \"California Love\" by 2Pac hai)। Phir `print_recommendations` function ko `song_id` 842 dekar run karta hai. Output mein jo songs recommend hote hain, woh Word2Vec model ke hisaab se \"California Love\" (2Pac) ke similar hain। Jab hum recommendations dekhte hain (jaise The Game, Nas, Kanye West), toh humein pata chalta hai ki yeh bhi Hip-Hop artists hain. Iska matlab hai ki Word2Vec model playlists ke data se seekh kar songs ke style ya genre ko bhi embeddings mein capture kar leta hai, jisse relevant recommendations milte hain। Embeddings ka power yahan साफ dikhta hai!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📚 Seekhe Hue Shabd (Glossary)\n",
    "\n",
    "Chalo, jo naye aur important technical shabd humne is Chapter mein seekhe, unka matlab aur thoda simple explanation aur analogies dekh lete hain, taaki yeh sab yaad rahe! Socho yeh hamari AI dictionary hai!\n",
    "\n",
    "*   **Tokens:** Bhasha (text) ke chote chote tukde jinmein AI model input ko todta hai. Yeh poore shabd ho sakte hain (\"cat\"), shabdon ke hisse (`apolog`, `izing`), punctuation (`.`, `:`), ya special symbols/characters। **Analogy:** Jaise building blocks ya LEGO pieces jinse poora sentence banta hai। Har token ek block hai!\n",
    "*   **Tokenization:** Woh process jismein Language Model ka tokenizer input text ko in chote chote **Tokens** mein todta hai। **Analogy:** Jaise koi complicated cheez ko chote chote parts mein divide karna usko study karne ya use karne ke liye, ya LEGO set ko uske pieces mein alag karna banane se pehle।\n",
    "*   **Tokenizer:** Woh tool ya software jo text ko tokens mein todta hai। **Analogy:** Woh machine ya person jo tumhari bhasha ko AI ke LEGO pieces mein badalta hai।\n",
    "*   **Token ID:** Har unique token ke liye ek special number. AI model tokens ko unke IDs se pehchanta hai aur numbers par kaam karta hai। **Analogy:** Jaise har LEGO piece ka apna ek unique number ya code ho jo uski pehchan hai। AI numbers (IDs) par kaam karta hai, text par nahi।\n",
    "*   **Embeddings:** Tokens (ya poore sentences/documents) ko represent karne wali numbers ki list (jisko technical language mein **vector** kehte hain). Yeh numbers token/text ke \"matlab\" ya meaning ko capture karte hain। **Analogy:** Har LEGO piece ka ek \"power level\" ya \"functionality score\" list, jo batata hai ki woh kis tarah ka piece hai aur kya kar sakta hai। Jin pieces ka kaam similar hai, unke scores paas paas honge।\n",
    "*   **Embedding Size / Vector Dimension:** Ek embedding vector mein kitne numbers hain, uski lambai। **Analogy:** Woh \"power level score card\" mein kitni entries hain (kitne alag alag tarah ke scores)।\n",
    "*   **Contextualized Embeddings:** Modern LLMs dwara banaye gaye embeddings jahan ek token ka meaning number uske aas paas ke shabdon (context) par depend karta hai। **Analogy:** Jaise \"bank\" shabd ka matlab meaning number list sentence ke hisaab se badal jayega - river bank wala bank ka number money bank wale bank se alag hoga।\n",
    "*   **Text Embeddings / Sentence Embeddings:** Ek single meaning number list (vector) jo poore sentence ya poore document ke combined matlab ko represent karta hai। **Analogy:** Jaise poore LEGO model ka ek overall score jo batata hai ki woh kis tarah ka model hai (car, building, etc.)। Yeh do models ko compare karne ke kaam aata hai।\n",
    "*   **Non-Contextual Embeddings:** Purane tarike ke embeddings jahan ek word ka meaning number hamesha same rehta hai, bhale hi woh kisi bhi context mein ho। **Analogy:** Jaise purani dictionary jismein har word ka ek hi entry ho, \"bank\" ka ek hi matlab ho numbers mein। Word2Vec aur GloVe iske examples hain।\n",
    "*   **Word2Vec:** Non-Contextual Word Embeddings banane ka ek popular algorithm jo shabdon ke aas paas ke shabdon (context) ke hisaab se numbers deta hai। **Analogy:** Jaise shabdon ki dosti ka score: jo shabd dost hain (aksar saath aate hain), unke numbers paas paas honge। Humne isko song recommendations ke liye use kiya!\n",
    "*   **GloVe:** Non-Contextual Word Embeddings banane ka ek aur popular algorithm। Yeh World2Vec se thoda alag tarike se kaam karta hai par idea similar hai। **Analogy:** Dosti ka score calculate karne ka ek doosra tarika।\n",
    "*   **Gensim:** Ek Python library jo Word Embeddings (jaise Word2Vec, GloVe) aur related text processing tasks ke liye tools deta hai। **Analogy:** Ek toolkit jismein Word Embeddings banane aur use karne ke saare auzaar hain।\n",
    "*   **Pandas:** Python mein data ko tables (DataFrames) mein organize karne ke liye ek bahut useful library। **Analogy:** Ek smart spreadsheet table jismein songs ki IDs ke saath unke naam aur artist likhe hain, aur hum ID se jaldi dhundh sakte hain।\n",
    "*   **DataFrame (Pandas):** Pandas library mein table-like data structure। Rows aur columns hote hain। **Analogy:** Wahi smart spreadsheet table jiske baare mein abhi baat ki।\n",
    "*   **GPU (Graphics Processing Unit):** Computer mein ek special chip jo bahut saari math calculations ek saath tez kar sakti hai। AI models ko train aur run karne ke liye bahut useful। **Analogy:** Ek Super Engine jo AI ke kaam ko Rocket 🚀 jaisa tez kar deta hai!\n",
    "\n",
    "---\n",
    "\n",
    "💡 **Picture Suggestion:** Yahan par ek image mein dictionary jaisi book dikha sakte ho, aur uske side mein ek graph jismein points (shabd) alag alag groups mein paas paas hon (jaise animals ka group, fruits ka group) aur un points ke paas numbers likhe hon। Isse Embeddings ka idea clear hoga।"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Summary: Kya Seekha Aur Aage Kya? 🎉\n",
    "\n",
    "Waah! Aaj humne AI ke andar ke kuch bahut important aur interesting concepts ko explore kiya। Socho hum AI ke dimag mein jhaank kar aaye hain! Humne seekha:\n",
    "\n",
    "*   AI models hamari bhasha ko chote chote **Tokens** mein todte hain (using a **Tokenizer**), jaise sentence ko LEGO pieces mein todna। Har token ka ek **Token ID** hota hai, jo uska unique code hai।\n",
    "*   Humne dekha ki alag alag models ke **Tokenizers** ek hi text ko alag alag tarike se handle karte hain (case, spaces, symbols), sabka apna apna style hai।\n",
    "*   Humne seekha ki **Embeddings** kya hote hain - tokens ya text ko represent karne wale meaning numbers (vectors)। Socho yeh har token ka 'matlab score card' hai।\n",
    "*   Humne **Contextualized Embeddings** (jaise BERT/DeBERTa se) ko dekha, jahan meaning numbers shabd ke sentence mein use hone ke context ke hisaab se badalte hain। \"Bank\" ka matlab badla, uska number badla!\n",
    "*   Humne **Text Embeddings** (Sentence-BERT se) ko dekha, jo poore sentence ya document ka ek single meaning number dete hain, do sentences ki similarity check karne ke liye perfect।\n",
    "*   Humne **Non-Contextual Embeddings** (jaise Word2Vec/GloVe) ko dekha, jo LLMs se pehle aate the aur jahan word ka embedding hamesha same rehta hai, bhale hi context kuch bhi ho।\n",
    "*   Aur sabse mazedaar baat, humne dekha ki kaise **embeddings** ka use karke hum real-world applications bana sakte hain, jaise song recommendations! 🎶 Yeh dikhata hai ki meaning numbers kitne useful ho sakte hain।\n",
    "\n",
    "Yeh Chapter AI ke Language understanding ki foundation hai. Tokens aur Embeddings woh tarika hai jisse computer hamari baaton ko numerical roop deta hai jispar calculations ki ja sakein aur AI \"samajh\" sake।\n",
    "\n",
    "Aage chapters mein, hum Language Models ke aur bhi andar jaayenge, unke architecture (woh kaise bane hain, unke layers) ko samjhenge, aur dekhenge ki kaise woh in embeddings ka use karke itne smart aur creative answers generate karte hain!\n",
    "\n",
    "Keep exploring and keep building with AI's LEGO blocks! 🧱🤖\n",
    "\n",
    "---\n",
    "**The End**\n",
    "---\n",
    "\n",
    "💡 **Picture Suggestion:** Yahan chapter ke end mein ek happy robot ki drawing ho sakti hai jo haath mein LEGO blocks liye hue hai, aur uske piche text aur numbers float kar rahe hon।"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
